---
title: "Taiyeb's 16S analysis - DADA2"
output: html_notebook
---


Jacobo de la Cuesta-Zuluaga. August 2022.

The aim of this notebook is to process the amplicon sequences from mice using DADA2.

# Libraries
```{r}
library(tidyverse)
library(dada2)
library(DECIPHER)
library(ape)
library(Biostrings)
library(conflicted)
```

```{r}
conflict_prefer("filter", "dplyr")
conflict_prefer("slice", "dplyr")
conflict_prefer("rename", "dplyr")
```


# Paths
```{r}
# Project directories
base_dir =  "Q:/NAS/AG_Maier/Amplicon_Sequencing"
out_dir = file.path(base_dir, "Projects/22_08_Nalini_Large_16S/dada2_out")
dir.create(out_dir)

# Raw sequences
seqs_dir = c("Q:/MK/ag_maier/Sequence_Data/2022_08/Nalini_samples")


# Filtered sequences
filt_seqs_dir = file.path(out_dir, "filtered_seqs")
dir.create(filt_seqs_dir)

# Reference dbs
ref_dir = file.path(base_dir, "Reference_files")

gtdb_tax = file.path(ref_dir, "gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna.gz")
gtdb_species = file.path(ref_dir, "gtdb-sbdi-sativa.r06rs202.addSpecies.fna.gz")

silva_tax = file.path(ref_dir, "silva_nr99_v138.1_train_set.fa.gz")
silva_species = file.path(ref_dir, "silva_species_assignment_v138.1.fa.gz")

COM20_species =  file.path(ref_dir, "COM20_GTDB_addSpecies.fna.gz")
COM20_ref_taxonomy = file.path(ref_dir, "COM20_GTDB_Taxonomy.txt") %>% 
  read_tsv() %>% 
  mutate(GTDB_Taxonomy = str_remove_all(GTDB_Taxonomy, "[a-z]__")) %>% 
  separate(GTDB_Taxonomy, into = c("Domain", "Phylum", "Class", "Order", "Family", "Genus", "Species"), sep = ";")
```

*Note* that I had to manually change the GTDB reference files.
They were origianlly obtained from https://scilifelab.figshare.com/articles/dataset/SBDI_Sativa_curated_16S_GTDB_database/14869077 (June 2022).
They have a double domain entry because the authors of these files require them in such way for their pipeline.
However, this leads to problems when used directly on dada2.
To fix, I used the following commands:

```
zcat gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fna.gz | sed -E 's~>Bacteria;|>Archaea;~>~' | sed '/>.*/s/$/;/' > gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna
gzip gtdb-sbdi-sativa.r06rs202.assignTaxonomy.fixed.fna
```


```{r}
# List files in raw sequences dir
forward_reads = map(seqs_dir, 
                    function (x) list.files(x, pattern="R1_001.fastq.gz", full.names = TRUE, recursive = FALSE)) %>% 
  unlist()
  
  
reverse_reads = map(seqs_dir, 
                    function (x) list.files(x, pattern="R2_001.fastq.gz", full.names = TRUE, recursive = FALSE)) %>% 
  unlist()

# Make sure the corresponding F and R files are present
forward_names = str_remove(forward_reads, "_R1_001.fastq.gz")
reverse_names = str_remove(reverse_reads, "_R2_001.fastq.gz")

setdiff(forward_names, reverse_names)
setdiff(reverse_names, forward_names)
```

# Execute DADA2
## Check quality of data
```{r warning=FALSE}
#Obtain quality plots of raw sequences for each of the files
Raw_QC_plotsF = map(forward_reads, function(fastq) {plotQualityProfile(fastq)})
Raw_QC_plotsR = map(reverse_reads, function(fastq) {plotQualityProfile(fastq)})
```

```{r}
# Print forward plots
Raw_QC_plotsF_hline = map(Raw_QC_plotsF, function(x) x + geom_hline(yintercept = 30))
Raw_QC_plotsF_hline[1:20]
```

```{r}
# Print Rreverse plots
Raw_QC_plotsR_hline = map(Raw_QC_plotsR, function(x) x + geom_hline(yintercept = 30))
Raw_QC_plotsR_hline[1:20]
```

## Determine positions to truncate
```{r}
# Function to calculate mean quality per position
Per_pos_mean = function(QC_df){
  QC_df %>% 
  group_by(Cycle) %>% 
  mutate(weight = Score * Count, 
         nbases = sum(Count)) %>% 
  summarize(mean_qual = sum(weight)/median(nbases)) %>% 
  ungroup()
}
```

```{r}
# Mean Quality DFs forward
mean_quals_F = map(Raw_QC_plotsF, function(qual_df){
  Per_pos_mean(qual_df$data)
})

# Mean Quality DFs reverse
mean_quals_R = map(Raw_QC_plotsR, function(qual_df){
  Per_pos_mean(qual_df$data)
})
```

```{r}
# Determine on each sample first position with quality < 31
first_drop_F = map_dbl(mean_quals_F, function(QC_df){
  QC_df %>% 
  filter(Cycle> 10, mean_qual < 35) %>% 
  arrange(Cycle) %>% 
  slice(1) %>% 
  pull(Cycle)
})

# Mean position with quality < 32
first_drop_F %>% 
  median()

# Determine on each sample first position with quality < 30
first_drop_R = map_dbl(mean_quals_R, function(QC_df){
  QC_df %>% 
  filter(Cycle> 10, mean_qual < 35) %>% 
  arrange(Cycle) %>% 
  slice(1) %>% 
  pull(Cycle)
})


first_drop_R %>% 
  median()
```

Based on these results, combined with the plots above and the information on the DADA2 tutorial, for these data, I will use a truncation values of **240** for the forward reads, and **200** for the reverse reads.

# Filter reads
```{r}
# Filtering
filtered_trimmed = filterAndTrim(fwd =forward_reads, 
              rev = reverse_reads, 
              filt = filt_seqs_dir, 
              filt.rev = filt_seqs_dir,
              trimLeft = c(23,24), # Trim nucleotides corresponding to primer sequences. To check align primers to raw seqs
              truncLen=c(240, 200), # Truncation position of F and R
              maxEE=c(2,2), # Number of expected errors
              truncQ=11, # Truncate reads at the first instance of a quality score less than
              rm.phix=TRUE, # filter matches against the phiX
              compress=TRUE, 
              verbose=TRUE, 
              multithread=FALSE)
```

#  Learn errors
```{r}
# List files in raw sequences dir
forward_filt_reads = list.files(filt_seqs_dir, pattern="R1_001.fastq.gz", full.names = TRUE)
reverse_filt_reads = list.files(filt_seqs_dir, pattern="R2_001.fastq.gz", full.names = TRUE)

# Make sure the corresponding F and R files are present
forward_names = str_remove(basename(forward_filt_reads), "_R1_001.fastq.gz")
reverse_names = str_remove(basename(reverse_filt_reads), "_R2_001.fastq.gz")

setdiff(forward_names, reverse_names)
setdiff(reverse_names, forward_names)

# Add names to vectors
names(forward_filt_reads) = forward_names
names(reverse_filt_reads) = reverse_names
```
## Forward
```{r}
# Learn error rates
set.seed(2112)
forward_errors = learnErrors(forward_filt_reads, 
                             nbases = 1e8, 
                             randomize=TRUE,
                             multithread=FALSE, 
                             verbose = TRUE)

```

```{r}
plotErrors(forward_errors, nominalQ=TRUE)
```

## Reverse
```{r}
reverse_errors = learnErrors(forward_filt_reads, 
                             nbases = 1e8, 
                             randomize=TRUE,
                             multithread=FALSE, 
                             verbose = TRUE)
```

```{r}
plotErrors(reverse_errors, nominalQ=TRUE)
```

# Infer sequence variants
## Forward
```{r}
forward_derep = map(forward_filt_reads, 
                    function(filt_fq){derepFastq(filt_fq, verbose = TRUE)})

forward_dada = map(forward_derep, 
                   function(derep_obj){dada(derep_obj, err=forward_errors, multithread=FALSE, verbose = TRUE)})
```

## Reverse
```{r}
reverse_derep = map(reverse_filt_reads, 
                    function(filt_fq){derepFastq(filt_fq, verbose = TRUE)})

reverse_dada = map(reverse_derep, 
                   function(derep_obj){dada(derep_obj, err=reverse_errors, multithread=FALSE, verbose = TRUE)})

```

## Write to disk

```{r}
# Forward
forward_tab = makeSequenceTable(forward_dada)
f_dada_file = file.path(out_dir, "f_dada_tab.rds")
saveRDS(forward_tab, f_dada_file)

# Reverse
reverse_tab = makeSequenceTable(reverse_dada)
r_dada_file = file.path(out_dir, "r_dada_tab.rds")
saveRDS(reverse_tab, r_dada_file)
```

# Merge paired reads
```{r}
merged_reads = mergePairs(forward_dada, 
                          forward_derep, 
                          reverse_dada, 
                          reverse_derep, 
                          verbose=TRUE)
```


# Construct sequence table
```{r}
seq_table_raw = makeSequenceTable(merged_reads)
dim(seq_table_raw)
table(nchar(getSequences(seq_table_raw)))
```
```{r}
# Remove sequences with length above or below expected size
seq_table = seq_table_raw[,nchar(colnames(seq_table_raw)) %in% 250:256]
dim(seq_table)
table(nchar(getSequences(seq_table)))
```


# Remove chimeras
```{r}
seq_table_dechimered <- removeBimeraDenovo(seq_table,
                                    method="consensus", 
                                    multithread=FALSE, 
                                    verbose=TRUE)
dim(seq_table_dechimered)

# Proportion of non-chimeras
sum(seq_table_dechimered)/sum(seq_table)
```

# Number of reads through the pipeline
```{r}
getN = function(x) sum(getUniques(x))


Sequence_counts_workflow = filtered_trimmed %>% 
  as.data.frame() %>% 
  bind_cols(map_dbl(forward_dada, getN), 
            map_dbl(reverse_dada, getN),
            map_dbl(merged_reads, getN),
            rowSums(seq_table_dechimered)) %>% 
  rownames_to_column("Sample")

colnames(Sequence_counts_workflow) = c("Sample", "input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")


# Print
Sequence_counts_workflow
```

```{r, fig.height=8, fig.width=7}
Sequence_counts_workflow %>% 
  pivot_longer(cols = -Sample, names_to = "Step", values_to = "n_reads") %>% 
  mutate(Step = factor(Step, levels = c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim"))) %>% 
  ggplot(aes(x = Step, y = n_reads, group = Sample, color = Sample)) +
    geom_point() +
    geom_line() +
    theme_light() +
    theme(legend.position = "none") +
    labs(x = "Workflow Step", y = "Number of reads")
```

# Assign taxonomy
```{r}
merged_taxonomy = assignTaxonomy(seq_table_dechimered, 
                       gtdb_tax,
                       minBoot=70,
                       tryRC = TRUE,
                       multithread=FALSE)

merged_taxonomy_sp_raw = addSpecies(merged_taxonomy, 
                                    gtdb_species, 
                                    verbose=TRUE)

unname(merged_taxonomy_sp_raw) %>% 
  as.data.frame()
```

```{r}
# Data frame with taxonomy
# Add md5 sum for shorter unique ID of sequence
# Ad ASV#### identifier for ease of use
merged_taxonomy_sp_tmp = merged_taxonomy_sp_raw %>% 
  as.data.frame() %>% 
  rownames_to_column("Seq") %>% 
  bind_cols(md5 = map_chr(.$Seq, function(x) digest::digest(x, algo = "md5"))) %>% 
  mutate(ID = 1:nrow(.),
         ID = str_glue("ASV{id}", id = str_pad(ID, width = 4, pad = "0")), 
         ID = as.character(ID)) %>% 
  relocate(ID, md5)

tax_lvl = merged_taxonomy_sp_tmp  %>% 
  select(-c(ID, md5, Seq)) %>% 
  mutate_all(function(x) if_else(is.na(x), 0, 1)) %>% 
  rowSums()


# Add full name
# Add unclassified in case of missing level
merged_taxonomy_sp = merged_taxonomy_sp_tmp %>% 
  mutate(full_levels = tax_lvl) %>% 
  mutate(Name = case_when(full_levels == 1 ~ str_c("Unclassified", Kingdom, sep = " "), 
                          full_levels == 2 ~ str_c("Unclassified", Phylum, sep = " "),
                          full_levels == 3 ~ str_c("Unclassified", Class, sep = " "),
                          full_levels == 4 ~ str_c("Unclassified", Order, sep = " "),
                          full_levels == 5 ~ str_c("Unclassified", Family, sep = " "),
                          full_levels == 6 ~ str_c("Unclassified", Genus, sep = " "),
                          full_levels == 7 ~ str_c(Genus, Species, sep = " "))) %>% 
  relocate(ID, Name) %>% 
  select(-full_levels)



merged_taxonomy_sp %>% 
  head()
```


# Construct phylogeny
From https://compbiocore.github.io/metagenomics-workshop/assets/DADA2_tutorial.html
```{r}
# Retrieve sequences
ASV_seqs = getSequences(seq_table_dechimered)
names(ASV_seqs) = ASV_seqs
```

```{r}
# Align sequences
ASV_align = DECIPHER::AlignSeqs(Biostrings::DNAStringSet(ASV_seqs))

# Change sequence alignment output into a phyDat structure
ASV_align_matrix = phangorn::phyDat(as(ASV_align, "matrix"), type="DNA")

# Create distance matrix
ASV_dist = phangorn::dist.ml(ASV_align_matrix)

#Perform Neighbor joining

ASV_NJ_tree = phangorn::NJ(ASV_dist) # Note, tip order != sequence order

#Internal maximum likelihood
ASV_ML_fit = phangorn::pml(ASV_NJ_tree, data = ASV_align_matrix)

# negative edges length changed to 0!
ASV_ML_mod = update(ASV_ML_fit, k=4, inv=0.2)
ASV_ML_mod = phangorn::optim.pml(ASV_ML_mod, 
                         model="GTR", 
                         optInv=TRUE, 
                         optGamma=TRUE,
                         rearrangement = "stochastic", 
                         control = phangorn::pml.control(trace = 0))
```


```{r}
# Save tree to object
ASV_ML_tree = ASV_ML_mod$tree
ASV_ML_tree %>% 
  class
```


# Save files
## Create data frames
```{r}
# Make sure sequence in column name corresponds to the taxonomy table
colnames(seq_table_dechimered) == merged_taxonomy_sp$Seq

# Create ASV table
# Replace names for ASV ID. Can also be changed to the md5
ASV_df = seq_table_dechimered %>% 
  as.data.frame() %>% 
  rownames_to_column("Sample")

colnames(ASV_df) = c("Sample", merged_taxonomy_sp$ID)

# Print
ASV_df %>% 
  head
```




## Write tables and tree to file
```{r}
out_asv_file = file.path(out_dir, "Nalini_large_ASV_table.tsv")
out_tax_file = file.path(out_dir, "Nalini_large_taxonomy_table.tsv")
out_tree_file = file.path(out_dir, "Nalini_large_tree.tre")

write_tsv(ASV_df, out_asv_file)
write_tsv(merged_taxonomy_sp, out_tax_file)
saveRDS(seq_table_dechimered, file.path(out_dir, "Nalini_large_ASVs.rds"))
saveRDS(merged_taxonomy_sp, file.path(out_dir, "Nalini_large_taxonomy.rds"))
ape::write.tree(ASV_ML_tree, file = out_tree_file)
save.image(file.path(out_dir, "Nalini_large_16S.RData"))
```

# Session Info
```{r}
sessionInfo()
```
